{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good morning everyone, My name is caleb spikes and I would like to present my Dat 350 final project. At first I chose to do my project on credit card fraud, however I found that with that my ML models just weren't compatible with the dataset so I had to pivpt. I chose to explore a dataset from Kaggle.com on popular baby names by using several machine learning models. The dataset covers a multitude of years and provides us with a variety of information: Year of Birth, Gender, Ethnicity, and the count of occurrences for each baby name which is the column I chose to predict.\n",
    "\n",
    "The Machine learning models I chose to use were: Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, Gradient Boosting, and Naive Bayes. Each of these models are so different from each other giving me different ways to view the dataset. Which in turn helped me to gain a deeper understanding on what has influenced baby name popularity over time. With this final project I hoped to learn how to showcase different types of machine learning models for real world situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load in our data set and explore the general structure of it, so we have a better understanding before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year of Birth  Gender Ethnicity Child's First Name  Count  Rank\n",
      "0           2011  FEMALE  HISPANIC          GERALDINE     13    75\n",
      "1           2011  FEMALE  HISPANIC                GIA     21    67\n",
      "2           2011  FEMALE  HISPANIC             GIANNA     49    42\n",
      "3           2011  FEMALE  HISPANIC            GISELLE     38    51\n",
      "4           2011  FEMALE  HISPANIC              GRACE     36    53\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Popular_Baby_Names.csv\")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll handle missing values, encode categorical data, remove outliers, and normalize/standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "label_encoders = {}\n",
    "for column in [\"Gender\", \"Ethnicity\", \"Child's First Name\"]:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "iso_forest = IsolationForest()\n",
    "outliers = iso_forest.fit_predict(data.drop([\"Year of Birth\", \"Count\", \"Rank\"], axis=1))\n",
    "data = data[outliers == 1]\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "outliers = lof.fit_predict(data.drop([\"Year of Birth\", \"Count\", \"Rank\"], axis=1))\n",
    "data = data[outliers == 1]\n",
    "\n",
    "X = data.drop([\"Count\", \"Rank\"], axis=1)\n",
    "y = data[\"Count\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is clean and prepared, we can build and run our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.07077326343381389\n",
      "Random Forest Score: 0.8387942332896461\n",
      "Decision Tree Score: 0.846002621231979\n",
      "K-Nearest Neighbors Score: 0.7916120576671035\n",
      "Gradient Boosting Score: 0.019003931847968544\n",
      "Naive Bayes Score: 0.047182175622542594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in [\"Local Outlier Factor\", \"Support Vector Machine\"]:\n",
    "        \n",
    "        continue\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f\"{name} Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression - 0.0708\n",
    " - Logistic Regression achieved a low score of 0.0708, indicating poor performance in predicting the count of popular baby names. Logistic regression assumes a linear relationship between all the columns and our target which is count. If the correlation between these are not linear than it would struggle. \n",
    "\n",
    "2. Random Forest - 0.8388 \n",
    " - Random Forest achieved a score of 0.8388, indicating moderate to good performance in predicting the count of popular baby names. Random Forest is an ensemble learning method that combines multiple decision trees, and it performed well in this case, suggesting its reliability. The main reason is because it is less prone to over fitting. \n",
    "\n",
    "3. Decision Tree - 0.8460\n",
    " - Decision Tree achieved a score of 0.8460, slightly higher than Random Forest. Decision Tree models the data using a tree-like structure and performed slightly better than Random Forest in this case. DT's have the ability to find underlying patterns in big data even tho it is prone to overfitting. \n",
    "\n",
    "4. K Nearest Neighbors - 0.7916\n",
    " - K-Nearest Neighbors achieved a score of 0.7916, indicating moderate performance. KNN is a non-parametric method used for classification and regression, and it performed reasonably well in this case. KNN needs similarity between features in the dataset to work to the best of its ability and in this case, all of our columns relate to each other.\n",
    "\n",
    "5. Gradient Boosting - 0.0190\n",
    " - Gradient Boosting achieved a very low score of 0.0190, indicating poor performance. Gradient Boosting is an ensemble technique that builds trees sequentially, and in this case, it performed poorly compared to other models. Since I had some outliers, this model wasn't able to perform well.\n",
    "\n",
    "6. Naive Bayes - 0.0472\n",
    " - Naive Bayes achieved a low score of 0.0472, indicating poor performance. Naive Bayes is a probabilistic classifier based on Bayes' theorem with strong (naive) independence assumptions between features. In this case, it did not perform well. This is because the features in my dataset are highly correlated which leads to failure. \n",
    "\n",
    "In summary, Decision Tree and Random Forest are the most reliable models based on their relatively high scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final steps:\n",
    " - Hyperparameter optimization\n",
    " - Compare different models using cross validation\n",
    " - Retrain the best models with the best hyperparameters and evaluate on a testing set that was set aside from the rest of the testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "data = pd.read_csv(\"Popular_Baby_Names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Count\", \"Rank\"], axis=1)\n",
    "y = data[\"Count\"]\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    \"Random Forest\": Pipeline([\n",
    "        (\"model\", RandomForestRegressor())\n",
    "    ]),\n",
    "    \"Decision Tree\": Pipeline([\n",
    "        (\"model\", DecisionTreeRegressor())\n",
    "    ]),\n",
    "    \"Gradient Boosting\": Pipeline([\n",
    "        (\"model\", GradientBoostingRegressor())\n",
    "    ]),\n",
    "    \"K-Nearest Neighbors\": Pipeline([\n",
    "        (\"model\", KNeighborsRegressor())\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [50, 100, 200],\n",
    "        \"model__max_depth\": [None, 10, 20]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [None, 10, 20]\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model__n_estimators\": [50, 100, 200],\n",
    "        \"model__max_depth\": [3, 5, 7]\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        \"model__n_neighbors\": [3, 5, 7]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearchCV for Random Forest...\n",
      "Best parameters for Random Forest: {'model__max_depth': None, 'model__n_estimators': 100}\n",
      "Performing GridSearchCV for Decision Tree...\n",
      "Best parameters for Decision Tree: {'model__max_depth': None}\n",
      "Performing GridSearchCV for Gradient Boosting...\n",
      "Best parameters for Gradient Boosting: {'model__max_depth': 7, 'model__n_estimators': 200}\n",
      "Performing GridSearchCV for K-Nearest Neighbors...\n",
      "Best parameters for K-Nearest Neighbors: {'model__n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "best_estimators = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Performing GridSearchCV for {name}...\")\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[name], cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_encoded, y)\n",
    "    best_estimators[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using GridSearchCV for each of my ML models, I was able to get the best hyperparameters for each:\n",
    "\n",
    "1. Random Forest:\n",
    " - For Random Forest, the best performing model was achieved with an unlimited depth for individual decision trees (max_depth=None) and 100 estimators (i.e., the number of decision trees in the forest). This suggests that a larger number of decision trees helps capture more complex relationships in the data, while not constraining the maximum depth of each tree leads to better model flexibility.\n",
    "\n",
    "2. Decision Tree: \n",
    " - For Decision Tree, the best model was obtained when allowing the tree to grow to its maximum depth without any constraint (max_depth=None). This indicates that the decision tree algorithm was able to find the optimal level of complexity for the dataset without any explicit limitation on the tree depth.\n",
    "\n",
    "3. Gradient Boosting:\n",
    " - Gradient Boosting achieved the best performance with a maximum tree depth of 7 (max_depth=7) and 200 estimators (n_estimators=200). This suggests that a moderately deep tree combined with a larger number of boosting iterations provides the best balance between model complexity and generalization performance.\n",
    "\n",
    "4. K-Nearest Neighbors\n",
    " - K-Nearest Neighbors performed optimally with 5 neighbors (n_neighbors=5). This means that considering the 5 nearest neighbors for each prediction yielded the best results in terms of minimizing the mean squared error.\n",
    "\n",
    "By fine-tuning the hyperparameters through grid search cross-validation, we were able to optimize the performance of each machine learning model and obtain insights into the most effective configurations for predicting popular baby names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
